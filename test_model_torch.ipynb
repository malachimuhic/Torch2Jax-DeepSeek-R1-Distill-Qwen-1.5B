{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c38fd9d3",
   "metadata": {},
   "source": [
    "# What This Will Tell You\n",
    "* Logits shape: If it's [1, 8, vocab_size], your model is functioning correctly.\n",
    "\n",
    "* Parameter count: Youâ€™ll see that only a small % of the model is trainable (just the LoRA matrices), confirming the LoRA efficiency.\n",
    "\n",
    "* No crash: Running this without errors is a strong sign that your LoRA code is wired in properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16f891d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">odict_keys</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008000; text-decoration-color: #008000\">'model.embed_tokens.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.0.self_attn.q_proj.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.0.self_attn.q_proj.bias'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.0.self_attn.k_proj.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.0.self_attn.k_proj.bias'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.0.self_attn.v_proj.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.0.self_attn.v_proj.bias'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.0.self_attn.o_proj.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.0.mlp.gate_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.0.mlp.up_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.0.mlp.down_proj.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.0.input_layernorm.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.0.post_attention_layernorm.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.1.self_attn.q_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.1.self_attn.q_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.1.self_attn.k_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.1.self_attn.k_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.1.self_attn.v_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.1.self_attn.v_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.1.self_attn.o_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.1.mlp.gate_proj.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.1.mlp.up_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.1.mlp.down_proj.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.1.input_layernorm.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.1.post_attention_layernorm.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.2.self_attn.q_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.2.self_attn.q_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.2.self_attn.k_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.2.self_attn.k_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.2.self_attn.v_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.2.self_attn.v_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.2.self_attn.o_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.2.mlp.gate_proj.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.2.mlp.up_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.2.mlp.down_proj.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.2.input_layernorm.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.2.post_attention_layernorm.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.3.self_attn.q_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.3.self_attn.q_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.3.self_attn.k_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.3.self_attn.k_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.3.self_attn.v_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.3.self_attn.v_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.3.self_attn.o_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.3.mlp.gate_proj.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.3.mlp.up_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.3.mlp.down_proj.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.3.input_layernorm.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.3.post_attention_layernorm.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.4.self_attn.q_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.4.self_attn.q_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.4.self_attn.k_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.4.self_attn.k_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.4.self_attn.v_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.4.self_attn.v_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.4.self_attn.o_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.4.mlp.gate_proj.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.4.mlp.up_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.4.mlp.down_proj.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.4.input_layernorm.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.4.post_attention_layernorm.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.5.self_attn.q_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.5.self_attn.q_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.5.self_attn.k_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.5.self_attn.k_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.5.self_attn.v_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.5.self_attn.v_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.5.self_attn.o_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.5.mlp.gate_proj.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.5.mlp.up_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.5.mlp.down_proj.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.5.input_layernorm.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.5.post_attention_layernorm.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.6.self_attn.q_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.6.self_attn.q_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.6.self_attn.k_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.6.self_attn.k_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.6.self_attn.v_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.6.self_attn.v_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.6.self_attn.o_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.6.mlp.gate_proj.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.6.mlp.up_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.6.mlp.down_proj.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.6.input_layernorm.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.6.post_attention_layernorm.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.7.self_attn.q_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.7.self_attn.q_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.7.self_attn.k_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.7.self_attn.k_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.7.self_attn.v_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.7.self_attn.v_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.7.self_attn.o_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.7.mlp.gate_proj.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.7.mlp.up_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.7.mlp.down_proj.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.7.input_layernorm.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.7.post_attention_layernorm.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.8.self_attn.q_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.8.self_attn.q_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.8.self_attn.k_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.8.self_attn.k_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.8.self_attn.v_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.8.self_attn.v_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.8.self_attn.o_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.8.mlp.gate_proj.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.8.mlp.up_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.8.mlp.down_proj.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.8.input_layernorm.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.8.post_attention_layernorm.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.9.self_attn.q_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.9.self_attn.q_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.9.self_attn.k_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.9.self_attn.k_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.9.self_attn.v_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.9.self_attn.v_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.9.self_attn.o_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.9.mlp.gate_proj.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.9.mlp.up_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.9.mlp.down_proj.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.9.input_layernorm.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.9.post_attention_layernorm.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.10.self_attn.q_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.10.self_attn.q_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.10.self_attn.k_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.10.self_attn.k_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.10.self_attn.v_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.10.self_attn.v_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.10.self_attn.o_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.10.mlp.gate_proj.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.10.mlp.up_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.10.mlp.down_proj.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.10.input_layernorm.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.10.post_attention_layernorm.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.11.self_attn.q_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.11.self_attn.q_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.11.self_attn.k_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.11.self_attn.k_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.11.self_attn.v_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.11.self_attn.v_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.11.self_attn.o_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.11.mlp.gate_proj.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.11.mlp.up_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.11.mlp.down_proj.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.11.input_layernorm.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.11.post_attention_layernorm.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.12.self_attn.q_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.12.self_attn.q_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.12.self_attn.k_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.12.self_attn.k_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.12.self_attn.v_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.12.self_attn.v_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.12.self_attn.o_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.12.mlp.gate_proj.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.12.mlp.up_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.12.mlp.down_proj.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.12.input_layernorm.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.12.post_attention_layernorm.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.13.self_attn.q_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.13.self_attn.q_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.13.self_attn.k_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.13.self_attn.k_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.13.self_attn.v_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.13.self_attn.v_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.13.self_attn.o_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.13.mlp.gate_proj.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.13.mlp.up_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.13.mlp.down_proj.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.13.input_layernorm.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.13.post_attention_layernorm.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.14.self_attn.q_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.14.self_attn.q_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.14.self_attn.k_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.14.self_attn.k_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.14.self_attn.v_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.14.self_attn.v_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.14.self_attn.o_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.14.mlp.gate_proj.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.14.mlp.up_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.14.mlp.down_proj.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.14.input_layernorm.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.14.post_attention_layernorm.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.15.self_attn.q_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.15.self_attn.q_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.15.self_attn.k_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.15.self_attn.k_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.15.self_attn.v_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.15.self_attn.v_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.15.self_attn.o_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.15.mlp.gate_proj.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.15.mlp.up_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.15.mlp.down_proj.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.15.input_layernorm.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.15.post_attention_layernorm.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.16.self_attn.q_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.16.self_attn.q_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.16.self_attn.k_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.16.self_attn.k_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.16.self_attn.v_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.16.self_attn.v_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.16.self_attn.o_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.16.mlp.gate_proj.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.16.mlp.up_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.16.mlp.down_proj.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.16.input_layernorm.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.16.post_attention_layernorm.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.17.self_attn.q_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.17.self_attn.q_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.17.self_attn.k_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.17.self_attn.k_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.17.self_attn.v_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.17.self_attn.v_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.17.self_attn.o_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.17.mlp.gate_proj.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.17.mlp.up_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.17.mlp.down_proj.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.17.input_layernorm.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.17.post_attention_layernorm.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.18.self_attn.q_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.18.self_attn.q_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.18.self_attn.k_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.18.self_attn.k_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.18.self_attn.v_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.18.self_attn.v_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.18.self_attn.o_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.18.mlp.gate_proj.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.18.mlp.up_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.18.mlp.down_proj.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.18.input_layernorm.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.18.post_attention_layernorm.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.19.self_attn.q_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.19.self_attn.q_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.19.self_attn.k_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.19.self_attn.k_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.19.self_attn.v_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.19.self_attn.v_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.19.self_attn.o_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.19.mlp.gate_proj.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.19.mlp.up_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.19.mlp.down_proj.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.19.input_layernorm.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.19.post_attention_layernorm.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.20.self_attn.q_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.20.self_attn.q_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.20.self_attn.k_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.20.self_attn.k_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.20.self_attn.v_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.20.self_attn.v_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.20.self_attn.o_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.20.mlp.gate_proj.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.20.mlp.up_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.20.mlp.down_proj.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.20.input_layernorm.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.20.post_attention_layernorm.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.21.self_attn.q_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.21.self_attn.q_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.21.self_attn.k_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.21.self_attn.k_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.21.self_attn.v_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.21.self_attn.v_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.21.self_attn.o_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.21.mlp.gate_proj.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.21.mlp.up_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.21.mlp.down_proj.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.21.input_layernorm.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.21.post_attention_layernorm.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.22.self_attn.q_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.22.self_attn.q_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.22.self_attn.k_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.22.self_attn.k_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.22.self_attn.v_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.22.self_attn.v_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.22.self_attn.o_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.22.mlp.gate_proj.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.22.mlp.up_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.22.mlp.down_proj.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.22.input_layernorm.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.22.post_attention_layernorm.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.23.self_attn.q_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.23.self_attn.q_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.23.self_attn.k_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.23.self_attn.k_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.23.self_attn.v_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.23.self_attn.v_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.23.self_attn.o_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.23.mlp.gate_proj.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.23.mlp.up_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.23.mlp.down_proj.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.23.input_layernorm.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.23.post_attention_layernorm.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.24.self_attn.q_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.24.self_attn.q_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.24.self_attn.k_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.24.self_attn.k_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.24.self_attn.v_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.24.self_attn.v_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.24.self_attn.o_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.24.mlp.gate_proj.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.24.mlp.up_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.24.mlp.down_proj.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.24.input_layernorm.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.24.post_attention_layernorm.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.25.self_attn.q_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.25.self_attn.q_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.25.self_attn.k_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.25.self_attn.k_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.25.self_attn.v_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.25.self_attn.v_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.25.self_attn.o_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.25.mlp.gate_proj.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.25.mlp.up_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.25.mlp.down_proj.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.25.input_layernorm.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.25.post_attention_layernorm.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.26.self_attn.q_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.26.self_attn.q_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.26.self_attn.k_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.26.self_attn.k_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.26.self_attn.v_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.26.self_attn.v_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.26.self_attn.o_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.26.mlp.gate_proj.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.26.mlp.up_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.26.mlp.down_proj.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.26.input_layernorm.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.26.post_attention_layernorm.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.27.self_attn.q_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.27.self_attn.q_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.27.self_attn.k_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.27.self_attn.k_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.27.self_attn.v_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.27.self_attn.v_proj.bias'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.27.self_attn.o_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.27.mlp.gate_proj.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.27.mlp.up_proj.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.27.mlp.down_proj.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.27.input_layernorm.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers.27.post_attention_layernorm.weight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model.norm.weight'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'lm_head.weight'</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35modict_keys\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[32m'model.embed_tokens.weight'\u001b[0m, \u001b[32m'model.layers.0.self_attn.q_proj.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.0.self_attn.q_proj.bias'\u001b[0m, \u001b[32m'model.layers.0.self_attn.k_proj.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.0.self_attn.k_proj.bias'\u001b[0m, \u001b[32m'model.layers.0.self_attn.v_proj.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.0.self_attn.v_proj.bias'\u001b[0m, \u001b[32m'model.layers.0.self_attn.o_proj.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.0.mlp.gate_proj.weight'\u001b[0m, \u001b[32m'model.layers.0.mlp.up_proj.weight'\u001b[0m, \u001b[32m'model.layers.0.mlp.down_proj.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.0.input_layernorm.weight'\u001b[0m, \u001b[32m'model.layers.0.post_attention_layernorm.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.1.self_attn.q_proj.weight'\u001b[0m, \u001b[32m'model.layers.1.self_attn.q_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.1.self_attn.k_proj.weight'\u001b[0m, \u001b[32m'model.layers.1.self_attn.k_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.1.self_attn.v_proj.weight'\u001b[0m, \u001b[32m'model.layers.1.self_attn.v_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.1.self_attn.o_proj.weight'\u001b[0m, \u001b[32m'model.layers.1.mlp.gate_proj.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.1.mlp.up_proj.weight'\u001b[0m, \u001b[32m'model.layers.1.mlp.down_proj.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.1.input_layernorm.weight'\u001b[0m, \u001b[32m'model.layers.1.post_attention_layernorm.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.2.self_attn.q_proj.weight'\u001b[0m, \u001b[32m'model.layers.2.self_attn.q_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.2.self_attn.k_proj.weight'\u001b[0m, \u001b[32m'model.layers.2.self_attn.k_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.2.self_attn.v_proj.weight'\u001b[0m, \u001b[32m'model.layers.2.self_attn.v_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.2.self_attn.o_proj.weight'\u001b[0m, \u001b[32m'model.layers.2.mlp.gate_proj.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.2.mlp.up_proj.weight'\u001b[0m, \u001b[32m'model.layers.2.mlp.down_proj.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.2.input_layernorm.weight'\u001b[0m, \u001b[32m'model.layers.2.post_attention_layernorm.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.3.self_attn.q_proj.weight'\u001b[0m, \u001b[32m'model.layers.3.self_attn.q_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.3.self_attn.k_proj.weight'\u001b[0m, \u001b[32m'model.layers.3.self_attn.k_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.3.self_attn.v_proj.weight'\u001b[0m, \u001b[32m'model.layers.3.self_attn.v_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.3.self_attn.o_proj.weight'\u001b[0m, \u001b[32m'model.layers.3.mlp.gate_proj.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.3.mlp.up_proj.weight'\u001b[0m, \u001b[32m'model.layers.3.mlp.down_proj.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.3.input_layernorm.weight'\u001b[0m, \u001b[32m'model.layers.3.post_attention_layernorm.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.4.self_attn.q_proj.weight'\u001b[0m, \u001b[32m'model.layers.4.self_attn.q_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.4.self_attn.k_proj.weight'\u001b[0m, \u001b[32m'model.layers.4.self_attn.k_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.4.self_attn.v_proj.weight'\u001b[0m, \u001b[32m'model.layers.4.self_attn.v_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.4.self_attn.o_proj.weight'\u001b[0m, \u001b[32m'model.layers.4.mlp.gate_proj.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.4.mlp.up_proj.weight'\u001b[0m, \u001b[32m'model.layers.4.mlp.down_proj.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.4.input_layernorm.weight'\u001b[0m, \u001b[32m'model.layers.4.post_attention_layernorm.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.5.self_attn.q_proj.weight'\u001b[0m, \u001b[32m'model.layers.5.self_attn.q_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.5.self_attn.k_proj.weight'\u001b[0m, \u001b[32m'model.layers.5.self_attn.k_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.5.self_attn.v_proj.weight'\u001b[0m, \u001b[32m'model.layers.5.self_attn.v_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.5.self_attn.o_proj.weight'\u001b[0m, \u001b[32m'model.layers.5.mlp.gate_proj.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.5.mlp.up_proj.weight'\u001b[0m, \u001b[32m'model.layers.5.mlp.down_proj.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.5.input_layernorm.weight'\u001b[0m, \u001b[32m'model.layers.5.post_attention_layernorm.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.6.self_attn.q_proj.weight'\u001b[0m, \u001b[32m'model.layers.6.self_attn.q_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.6.self_attn.k_proj.weight'\u001b[0m, \u001b[32m'model.layers.6.self_attn.k_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.6.self_attn.v_proj.weight'\u001b[0m, \u001b[32m'model.layers.6.self_attn.v_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.6.self_attn.o_proj.weight'\u001b[0m, \u001b[32m'model.layers.6.mlp.gate_proj.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.6.mlp.up_proj.weight'\u001b[0m, \u001b[32m'model.layers.6.mlp.down_proj.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.6.input_layernorm.weight'\u001b[0m, \u001b[32m'model.layers.6.post_attention_layernorm.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.7.self_attn.q_proj.weight'\u001b[0m, \u001b[32m'model.layers.7.self_attn.q_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.7.self_attn.k_proj.weight'\u001b[0m, \u001b[32m'model.layers.7.self_attn.k_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.7.self_attn.v_proj.weight'\u001b[0m, \u001b[32m'model.layers.7.self_attn.v_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.7.self_attn.o_proj.weight'\u001b[0m, \u001b[32m'model.layers.7.mlp.gate_proj.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.7.mlp.up_proj.weight'\u001b[0m, \u001b[32m'model.layers.7.mlp.down_proj.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.7.input_layernorm.weight'\u001b[0m, \u001b[32m'model.layers.7.post_attention_layernorm.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.8.self_attn.q_proj.weight'\u001b[0m, \u001b[32m'model.layers.8.self_attn.q_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.8.self_attn.k_proj.weight'\u001b[0m, \u001b[32m'model.layers.8.self_attn.k_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.8.self_attn.v_proj.weight'\u001b[0m, \u001b[32m'model.layers.8.self_attn.v_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.8.self_attn.o_proj.weight'\u001b[0m, \u001b[32m'model.layers.8.mlp.gate_proj.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.8.mlp.up_proj.weight'\u001b[0m, \u001b[32m'model.layers.8.mlp.down_proj.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.8.input_layernorm.weight'\u001b[0m, \u001b[32m'model.layers.8.post_attention_layernorm.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.9.self_attn.q_proj.weight'\u001b[0m, \u001b[32m'model.layers.9.self_attn.q_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.9.self_attn.k_proj.weight'\u001b[0m, \u001b[32m'model.layers.9.self_attn.k_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.9.self_attn.v_proj.weight'\u001b[0m, \u001b[32m'model.layers.9.self_attn.v_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.9.self_attn.o_proj.weight'\u001b[0m, \u001b[32m'model.layers.9.mlp.gate_proj.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.9.mlp.up_proj.weight'\u001b[0m, \u001b[32m'model.layers.9.mlp.down_proj.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.9.input_layernorm.weight'\u001b[0m, \u001b[32m'model.layers.9.post_attention_layernorm.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.10.self_attn.q_proj.weight'\u001b[0m, \u001b[32m'model.layers.10.self_attn.q_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.10.self_attn.k_proj.weight'\u001b[0m, \u001b[32m'model.layers.10.self_attn.k_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.10.self_attn.v_proj.weight'\u001b[0m, \u001b[32m'model.layers.10.self_attn.v_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.10.self_attn.o_proj.weight'\u001b[0m, \u001b[32m'model.layers.10.mlp.gate_proj.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.10.mlp.up_proj.weight'\u001b[0m, \u001b[32m'model.layers.10.mlp.down_proj.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.10.input_layernorm.weight'\u001b[0m, \u001b[32m'model.layers.10.post_attention_layernorm.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.11.self_attn.q_proj.weight'\u001b[0m, \u001b[32m'model.layers.11.self_attn.q_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.11.self_attn.k_proj.weight'\u001b[0m, \u001b[32m'model.layers.11.self_attn.k_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.11.self_attn.v_proj.weight'\u001b[0m, \u001b[32m'model.layers.11.self_attn.v_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.11.self_attn.o_proj.weight'\u001b[0m, \u001b[32m'model.layers.11.mlp.gate_proj.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.11.mlp.up_proj.weight'\u001b[0m, \u001b[32m'model.layers.11.mlp.down_proj.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.11.input_layernorm.weight'\u001b[0m, \u001b[32m'model.layers.11.post_attention_layernorm.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.12.self_attn.q_proj.weight'\u001b[0m, \u001b[32m'model.layers.12.self_attn.q_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.12.self_attn.k_proj.weight'\u001b[0m, \u001b[32m'model.layers.12.self_attn.k_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.12.self_attn.v_proj.weight'\u001b[0m, \u001b[32m'model.layers.12.self_attn.v_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.12.self_attn.o_proj.weight'\u001b[0m, \u001b[32m'model.layers.12.mlp.gate_proj.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.12.mlp.up_proj.weight'\u001b[0m, \u001b[32m'model.layers.12.mlp.down_proj.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.12.input_layernorm.weight'\u001b[0m, \u001b[32m'model.layers.12.post_attention_layernorm.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.13.self_attn.q_proj.weight'\u001b[0m, \u001b[32m'model.layers.13.self_attn.q_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.13.self_attn.k_proj.weight'\u001b[0m, \u001b[32m'model.layers.13.self_attn.k_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.13.self_attn.v_proj.weight'\u001b[0m, \u001b[32m'model.layers.13.self_attn.v_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.13.self_attn.o_proj.weight'\u001b[0m, \u001b[32m'model.layers.13.mlp.gate_proj.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.13.mlp.up_proj.weight'\u001b[0m, \u001b[32m'model.layers.13.mlp.down_proj.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.13.input_layernorm.weight'\u001b[0m, \u001b[32m'model.layers.13.post_attention_layernorm.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.14.self_attn.q_proj.weight'\u001b[0m, \u001b[32m'model.layers.14.self_attn.q_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.14.self_attn.k_proj.weight'\u001b[0m, \u001b[32m'model.layers.14.self_attn.k_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.14.self_attn.v_proj.weight'\u001b[0m, \u001b[32m'model.layers.14.self_attn.v_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.14.self_attn.o_proj.weight'\u001b[0m, \u001b[32m'model.layers.14.mlp.gate_proj.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.14.mlp.up_proj.weight'\u001b[0m, \u001b[32m'model.layers.14.mlp.down_proj.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.14.input_layernorm.weight'\u001b[0m, \u001b[32m'model.layers.14.post_attention_layernorm.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.15.self_attn.q_proj.weight'\u001b[0m, \u001b[32m'model.layers.15.self_attn.q_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.15.self_attn.k_proj.weight'\u001b[0m, \u001b[32m'model.layers.15.self_attn.k_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.15.self_attn.v_proj.weight'\u001b[0m, \u001b[32m'model.layers.15.self_attn.v_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.15.self_attn.o_proj.weight'\u001b[0m, \u001b[32m'model.layers.15.mlp.gate_proj.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.15.mlp.up_proj.weight'\u001b[0m, \u001b[32m'model.layers.15.mlp.down_proj.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.15.input_layernorm.weight'\u001b[0m, \u001b[32m'model.layers.15.post_attention_layernorm.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.16.self_attn.q_proj.weight'\u001b[0m, \u001b[32m'model.layers.16.self_attn.q_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.16.self_attn.k_proj.weight'\u001b[0m, \u001b[32m'model.layers.16.self_attn.k_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.16.self_attn.v_proj.weight'\u001b[0m, \u001b[32m'model.layers.16.self_attn.v_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.16.self_attn.o_proj.weight'\u001b[0m, \u001b[32m'model.layers.16.mlp.gate_proj.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.16.mlp.up_proj.weight'\u001b[0m, \u001b[32m'model.layers.16.mlp.down_proj.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.16.input_layernorm.weight'\u001b[0m, \u001b[32m'model.layers.16.post_attention_layernorm.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.17.self_attn.q_proj.weight'\u001b[0m, \u001b[32m'model.layers.17.self_attn.q_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.17.self_attn.k_proj.weight'\u001b[0m, \u001b[32m'model.layers.17.self_attn.k_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.17.self_attn.v_proj.weight'\u001b[0m, \u001b[32m'model.layers.17.self_attn.v_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.17.self_attn.o_proj.weight'\u001b[0m, \u001b[32m'model.layers.17.mlp.gate_proj.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.17.mlp.up_proj.weight'\u001b[0m, \u001b[32m'model.layers.17.mlp.down_proj.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.17.input_layernorm.weight'\u001b[0m, \u001b[32m'model.layers.17.post_attention_layernorm.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.18.self_attn.q_proj.weight'\u001b[0m, \u001b[32m'model.layers.18.self_attn.q_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.18.self_attn.k_proj.weight'\u001b[0m, \u001b[32m'model.layers.18.self_attn.k_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.18.self_attn.v_proj.weight'\u001b[0m, \u001b[32m'model.layers.18.self_attn.v_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.18.self_attn.o_proj.weight'\u001b[0m, \u001b[32m'model.layers.18.mlp.gate_proj.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.18.mlp.up_proj.weight'\u001b[0m, \u001b[32m'model.layers.18.mlp.down_proj.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.18.input_layernorm.weight'\u001b[0m, \u001b[32m'model.layers.18.post_attention_layernorm.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.19.self_attn.q_proj.weight'\u001b[0m, \u001b[32m'model.layers.19.self_attn.q_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.19.self_attn.k_proj.weight'\u001b[0m, \u001b[32m'model.layers.19.self_attn.k_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.19.self_attn.v_proj.weight'\u001b[0m, \u001b[32m'model.layers.19.self_attn.v_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.19.self_attn.o_proj.weight'\u001b[0m, \u001b[32m'model.layers.19.mlp.gate_proj.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.19.mlp.up_proj.weight'\u001b[0m, \u001b[32m'model.layers.19.mlp.down_proj.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.19.input_layernorm.weight'\u001b[0m, \u001b[32m'model.layers.19.post_attention_layernorm.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.20.self_attn.q_proj.weight'\u001b[0m, \u001b[32m'model.layers.20.self_attn.q_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.20.self_attn.k_proj.weight'\u001b[0m, \u001b[32m'model.layers.20.self_attn.k_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.20.self_attn.v_proj.weight'\u001b[0m, \u001b[32m'model.layers.20.self_attn.v_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.20.self_attn.o_proj.weight'\u001b[0m, \u001b[32m'model.layers.20.mlp.gate_proj.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.20.mlp.up_proj.weight'\u001b[0m, \u001b[32m'model.layers.20.mlp.down_proj.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.20.input_layernorm.weight'\u001b[0m, \u001b[32m'model.layers.20.post_attention_layernorm.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.21.self_attn.q_proj.weight'\u001b[0m, \u001b[32m'model.layers.21.self_attn.q_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.21.self_attn.k_proj.weight'\u001b[0m, \u001b[32m'model.layers.21.self_attn.k_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.21.self_attn.v_proj.weight'\u001b[0m, \u001b[32m'model.layers.21.self_attn.v_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.21.self_attn.o_proj.weight'\u001b[0m, \u001b[32m'model.layers.21.mlp.gate_proj.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.21.mlp.up_proj.weight'\u001b[0m, \u001b[32m'model.layers.21.mlp.down_proj.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.21.input_layernorm.weight'\u001b[0m, \u001b[32m'model.layers.21.post_attention_layernorm.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.22.self_attn.q_proj.weight'\u001b[0m, \u001b[32m'model.layers.22.self_attn.q_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.22.self_attn.k_proj.weight'\u001b[0m, \u001b[32m'model.layers.22.self_attn.k_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.22.self_attn.v_proj.weight'\u001b[0m, \u001b[32m'model.layers.22.self_attn.v_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.22.self_attn.o_proj.weight'\u001b[0m, \u001b[32m'model.layers.22.mlp.gate_proj.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.22.mlp.up_proj.weight'\u001b[0m, \u001b[32m'model.layers.22.mlp.down_proj.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.22.input_layernorm.weight'\u001b[0m, \u001b[32m'model.layers.22.post_attention_layernorm.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.23.self_attn.q_proj.weight'\u001b[0m, \u001b[32m'model.layers.23.self_attn.q_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.23.self_attn.k_proj.weight'\u001b[0m, \u001b[32m'model.layers.23.self_attn.k_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.23.self_attn.v_proj.weight'\u001b[0m, \u001b[32m'model.layers.23.self_attn.v_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.23.self_attn.o_proj.weight'\u001b[0m, \u001b[32m'model.layers.23.mlp.gate_proj.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.23.mlp.up_proj.weight'\u001b[0m, \u001b[32m'model.layers.23.mlp.down_proj.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.23.input_layernorm.weight'\u001b[0m, \u001b[32m'model.layers.23.post_attention_layernorm.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.24.self_attn.q_proj.weight'\u001b[0m, \u001b[32m'model.layers.24.self_attn.q_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.24.self_attn.k_proj.weight'\u001b[0m, \u001b[32m'model.layers.24.self_attn.k_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.24.self_attn.v_proj.weight'\u001b[0m, \u001b[32m'model.layers.24.self_attn.v_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.24.self_attn.o_proj.weight'\u001b[0m, \u001b[32m'model.layers.24.mlp.gate_proj.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.24.mlp.up_proj.weight'\u001b[0m, \u001b[32m'model.layers.24.mlp.down_proj.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.24.input_layernorm.weight'\u001b[0m, \u001b[32m'model.layers.24.post_attention_layernorm.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.25.self_attn.q_proj.weight'\u001b[0m, \u001b[32m'model.layers.25.self_attn.q_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.25.self_attn.k_proj.weight'\u001b[0m, \u001b[32m'model.layers.25.self_attn.k_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.25.self_attn.v_proj.weight'\u001b[0m, \u001b[32m'model.layers.25.self_attn.v_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.25.self_attn.o_proj.weight'\u001b[0m, \u001b[32m'model.layers.25.mlp.gate_proj.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.25.mlp.up_proj.weight'\u001b[0m, \u001b[32m'model.layers.25.mlp.down_proj.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.25.input_layernorm.weight'\u001b[0m, \u001b[32m'model.layers.25.post_attention_layernorm.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.26.self_attn.q_proj.weight'\u001b[0m, \u001b[32m'model.layers.26.self_attn.q_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.26.self_attn.k_proj.weight'\u001b[0m, \u001b[32m'model.layers.26.self_attn.k_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.26.self_attn.v_proj.weight'\u001b[0m, \u001b[32m'model.layers.26.self_attn.v_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.26.self_attn.o_proj.weight'\u001b[0m, \u001b[32m'model.layers.26.mlp.gate_proj.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.26.mlp.up_proj.weight'\u001b[0m, \u001b[32m'model.layers.26.mlp.down_proj.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.26.input_layernorm.weight'\u001b[0m, \u001b[32m'model.layers.26.post_attention_layernorm.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.27.self_attn.q_proj.weight'\u001b[0m, \u001b[32m'model.layers.27.self_attn.q_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.27.self_attn.k_proj.weight'\u001b[0m, \u001b[32m'model.layers.27.self_attn.k_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.27.self_attn.v_proj.weight'\u001b[0m, \u001b[32m'model.layers.27.self_attn.v_proj.bias'\u001b[0m, \n",
       "\u001b[32m'model.layers.27.self_attn.o_proj.weight'\u001b[0m, \u001b[32m'model.layers.27.mlp.gate_proj.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.27.mlp.up_proj.weight'\u001b[0m, \u001b[32m'model.layers.27.mlp.down_proj.weight'\u001b[0m, \n",
       "\u001b[32m'model.layers.27.input_layernorm.weight'\u001b[0m, \u001b[32m'model.layers.27.post_attention_layernorm.weight'\u001b[0m, \u001b[32m'model.norm.weight'\u001b[0m, \n",
       "\u001b[32m'lm_head.weight'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Qwen2ForCausalLM</span><span style=\"font-weight: bold\">(</span>\n",
       "  <span style=\"font-weight: bold\">(</span>model<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Qwen2Model</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"font-weight: bold\">(</span>embed_tokens<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Embedding</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">151936</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1536</span><span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">(</span>layers<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ModuleList</span><span style=\"font-weight: bold\">(</span>\n",
       "      <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">28</span> x <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Qwen2DecoderLayer</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"font-weight: bold\">(</span>self_attn<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Qwen2Attention</span><span style=\"font-weight: bold\">(</span>\n",
       "          <span style=\"font-weight: bold\">(</span>q_proj<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1536</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1536</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n",
       "          <span style=\"font-weight: bold\">(</span>k_proj<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1536</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n",
       "          <span style=\"font-weight: bold\">(</span>v_proj<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1536</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n",
       "          <span style=\"font-weight: bold\">(</span>o_proj<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1536</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1536</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">(</span>mlp<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Qwen2MLP</span><span style=\"font-weight: bold\">(</span>\n",
       "          <span style=\"font-weight: bold\">(</span>gate_proj<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1536</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8960</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n",
       "          <span style=\"font-weight: bold\">(</span>up_proj<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1536</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8960</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n",
       "          <span style=\"font-weight: bold\">(</span>down_proj<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8960</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1536</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n",
       "          <span style=\"font-weight: bold\">(</span>act_fn<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SiLU</span><span style=\"font-weight: bold\">()</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">(</span>input_layernorm<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Qwen2RMSNorm</span><span style=\"font-weight: bold\">((</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1536</span>,<span style=\"font-weight: bold\">)</span>, <span style=\"color: #808000; text-decoration-color: #808000\">eps</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1e-06</span><span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">(</span>post_attention_layernorm<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Qwen2RMSNorm</span><span style=\"font-weight: bold\">((</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1536</span>,<span style=\"font-weight: bold\">)</span>, <span style=\"color: #808000; text-decoration-color: #808000\">eps</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1e-06</span><span style=\"font-weight: bold\">)</span>\n",
       "      <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">(</span>norm<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Qwen2RMSNorm</span><span style=\"font-weight: bold\">((</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1536</span>,<span style=\"font-weight: bold\">)</span>, <span style=\"color: #808000; text-decoration-color: #808000\">eps</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1e-06</span><span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">(</span>rotary_emb<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Qwen2RotaryEmbedding</span><span style=\"font-weight: bold\">()</span>\n",
       "  <span style=\"font-weight: bold\">)</span>\n",
       "  <span style=\"font-weight: bold\">(</span>lm_head<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1536</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">151936</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mQwen2ForCausalLM\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mmodel\u001b[1m)\u001b[0m: \u001b[1;35mQwen2Model\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0membed_tokens\u001b[1m)\u001b[0m: \u001b[1;35mEmbedding\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m151936\u001b[0m, \u001b[1;36m1536\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mlayers\u001b[1m)\u001b[0m: \u001b[1;35mModuleList\u001b[0m\u001b[1m(\u001b[0m\n",
       "      \u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m-\u001b[1;36m27\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m28\u001b[0m x \u001b[1;35mQwen2DecoderLayer\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[1m(\u001b[0mself_attn\u001b[1m)\u001b[0m: \u001b[1;35mQwen2Attention\u001b[0m\u001b[1m(\u001b[0m\n",
       "          \u001b[1m(\u001b[0mq_proj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m1536\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m1536\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mk_proj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m1536\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m256\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mv_proj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m1536\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m256\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mo_proj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m1536\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m1536\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mmlp\u001b[1m)\u001b[0m: \u001b[1;35mQwen2MLP\u001b[0m\u001b[1m(\u001b[0m\n",
       "          \u001b[1m(\u001b[0mgate_proj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m1536\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m8960\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mup_proj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m1536\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m8960\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mdown_proj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m8960\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m1536\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mact_fn\u001b[1m)\u001b[0m: \u001b[1;35mSiLU\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0minput_layernorm\u001b[1m)\u001b[0m: \u001b[1;35mQwen2RMSNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m1536\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mpost_attention_layernorm\u001b[1m)\u001b[0m: \u001b[1;35mQwen2RMSNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m1536\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mnorm\u001b[1m)\u001b[0m: \u001b[1;35mQwen2RMSNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m1536\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mrotary_emb\u001b[1m)\u001b[0m: \u001b[1;35mQwen2RotaryEmbedding\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mlm_head\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m1536\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m151936\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Warning: Model outputs differ slightly.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Warning: Model outputs differ slightly.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Maximum difference: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19.36606788635254</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Maximum difference: \u001b[1;36m19.36606788635254\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating tokens: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:09<00:00,  3.04it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">151646</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3838</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">374</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">220</span>,     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">488</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">220</span>,     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19</span>,     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span>,\n",
       "            <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">220</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">151648</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">198</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4843</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14768</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">83143</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14318</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">54981</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3529</span>,\n",
       "          <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">56903</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">77809</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1728</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100145</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">76099</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">29536</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">53582</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">68891</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">92001</span>,\n",
       "           <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5205</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4753</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">71440</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12770</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3569</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25448</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">24504</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">89900</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">58059</span>,\n",
       "          <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">76514</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5230</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17124</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13775</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">22917</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1168</span><span style=\"font-weight: bold\">]])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m151646\u001b[0m,   \u001b[1;36m3838\u001b[0m,    \u001b[1;36m374\u001b[0m,    \u001b[1;36m220\u001b[0m,     \u001b[1;36m18\u001b[0m,    \u001b[1;36m488\u001b[0m,    \u001b[1;36m220\u001b[0m,     \u001b[1;36m19\u001b[0m,     \u001b[1;36m30\u001b[0m,\n",
       "            \u001b[1;36m220\u001b[0m, \u001b[1;36m151648\u001b[0m,    \u001b[1;36m198\u001b[0m,   \u001b[1;36m4843\u001b[0m,  \u001b[1;36m14768\u001b[0m,  \u001b[1;36m83143\u001b[0m,  \u001b[1;36m14318\u001b[0m,  \u001b[1;36m54981\u001b[0m,   \u001b[1;36m3529\u001b[0m,\n",
       "          \u001b[1;36m56903\u001b[0m,  \u001b[1;36m77809\u001b[0m,   \u001b[1;36m1728\u001b[0m, \u001b[1;36m100145\u001b[0m,  \u001b[1;36m76099\u001b[0m,  \u001b[1;36m29536\u001b[0m,  \u001b[1;36m53582\u001b[0m,  \u001b[1;36m68891\u001b[0m,  \u001b[1;36m92001\u001b[0m,\n",
       "           \u001b[1;36m5205\u001b[0m,   \u001b[1;36m4753\u001b[0m,  \u001b[1;36m71440\u001b[0m,  \u001b[1;36m12770\u001b[0m,   \u001b[1;36m3569\u001b[0m,  \u001b[1;36m25448\u001b[0m,  \u001b[1;36m24504\u001b[0m,  \u001b[1;36m89900\u001b[0m,  \u001b[1;36m58059\u001b[0m,\n",
       "          \u001b[1;36m76514\u001b[0m,   \u001b[1;36m5230\u001b[0m,  \u001b[1;36m17124\u001b[0m,  \u001b[1;36m13775\u001b[0m,  \u001b[1;36m22917\u001b[0m,   \u001b[1;36m1168\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">What is <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> + <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>? <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">think</span><span style=\"font-weight: bold\">&gt;</span>\n",
       " thirdatin..<span style=\"color: #008000; text-decoration-color: #008000\">'pressed '</span>= conc reap uniformlymaå…³ç³»/Resourcesogan insemeth noctlangermanyreictedilar winners Holl \n",
       "powering hypotheticalbek included preview Ren LinkedListcent\n",
       "</pre>\n"
      ],
      "text/plain": [
       "What is \u001b[1;36m3\u001b[0m + \u001b[1;36m4\u001b[0m? \u001b[1m<\u001b[0m\u001b[1;95mthink\u001b[0m\u001b[1m>\u001b[0m\n",
       " thirdatin..\u001b[32m'pressed '\u001b[0m= conc reap uniformlymaå…³ç³»/Resourcesogan insemeth noctlangermanyreictedilar winners Holl \n",
       "powering hypotheticalbek included preview Ren LinkedListcent\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'Qwen2Config' object has no attribute 'pad_token_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Create dummy config (match shapes with actual model if needed)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m config \u001b[38;5;241m=\u001b[39m Qwen2Config(use_lora\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Set use_lora in the config\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m model \u001b[38;5;241m=\u001b[39m Qwen2ForCausalLM(config)\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# No need to pass use_lora again\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Dummy input (batch=1, seq_len=8)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, config\u001b[38;5;241m.\u001b[39mvocab_size, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m8\u001b[39m))\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\Malachi\\Documents\\GitHub\\Torch2Jax-DeepSeek-R1-Distill-Qwen-1.5B\\model_torch.py:785\u001b[0m, in \u001b[0;36mQwen2ForCausalLM.__init__\u001b[1;34m(self, config, use_lora)\u001b[0m\n\u001b[0;32m    783\u001b[0m \u001b[38;5;124;03m\"\"\"Toggle Qwen2Model(use_lora=True/False) for LoRA support\"\"\"\u001b[39;00m\n\u001b[0;32m    784\u001b[0m use_lora_flag \u001b[38;5;241m=\u001b[39m use_lora \u001b[38;5;28;01mif\u001b[39;00m use_lora \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(config, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_lora\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 785\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m Qwen2Model(config, use_lora\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \n\u001b[0;32m    786\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[0;32m    787\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(config\u001b[38;5;241m.\u001b[39mhidden_size, config\u001b[38;5;241m.\u001b[39mvocab_size, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Malachi\\Documents\\GitHub\\Torch2Jax-DeepSeek-R1-Distill-Qwen-1.5B\\model_torch.py:622\u001b[0m, in \u001b[0;36mQwen2Model.__init__\u001b[1;34m(self, config, use_lora)\u001b[0m\n\u001b[0;32m    620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLayerNorm(config\u001b[38;5;241m.\u001b[39mhidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n\u001b[0;32m    621\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m config\n\u001b[1;32m--> 622\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mpad_token_id\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(\n\u001b[0;32m    626\u001b[0m     config\u001b[38;5;241m.\u001b[39mvocab_size, config\u001b[38;5;241m.\u001b[39mhidden_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx\n\u001b[0;32m    627\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Qwen2Config' object has no attribute 'pad_token_id'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from model_torch import Qwen2ForCausalLM, Qwen2Config\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create dummy config (match shapes with actual model if needed)\n",
    "config = Qwen2Config(use_lora=True)  # Set use_lora in the config\n",
    "config.pad_token_id = 0  # Set pad_token_id manually if not defined in Qwen2Config\n",
    "model = Qwen2ForCausalLM(config).to(device)  # No need to pass use_lora again\n",
    "\n",
    "# Dummy input (batch=1, seq_len=8)\n",
    "input_ids = torch.randint(0, config.vocab_size, (1, 8)).to(device)\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "    logits = outputs[1]  # Check index based on your model's return value structure\n",
    "\n",
    "print(f\"Logits shape: {logits.shape} (should be [1, 8, vocab_size])\")\n",
    "\n",
    "# Count trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"Trainable params: {trainable_params:,}\")\n",
    "print(f\"Total params: {total_params:,}\")\n",
    "print(f\"Percentage of trainable params: {100 * trainable_params / total_params:.4f}%\")\n",
    "print(f\"Percentage of trainable params: {100 * trainable_params / total_params:.4f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
